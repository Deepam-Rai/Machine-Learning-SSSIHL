{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Review Questions-I from MDSC-301(P)**\n",
    "**Assignment - II**\n",
    "------------------------------------------------\n",
    "**Author: Deepam Rai, 21231, II MSc**\n",
    "\n",
    "Date: September 5, 2022\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Which Linear Regression training algorithm can you use if you have\n",
    "a training set with millions of features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q2. Suppose the features in your training set have very different scales.\n",
    "Which algorithms might suffer from this, and how? What can you\n",
    "do about it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms that suffers:** Gradient Descent, PCA, etc because the feature with smaller scale will have very small value while larger scale may have very huge values due to which the smaller scale feature may get considered as insignificant or the iterative algorithms may keep meandering on the large scale feture taking very long time to reach the minima/maxima.   \n",
    "**Solution:** We have to scale the features, commonly we do:\n",
    "1. **Standardadization**  \n",
    "$ x_{std} = {x-\\bar{x}\\over s}$, where $\\bar{x}$ is the mean and $s$ is the standard deviation of x.  \n",
    "1. **Normalization**  \n",
    "$x_{normalized}={x-x_{min}\\over x_{max}-x_{min}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q3.  Suppose you use Batch Gradient Descent and you plot the validation\n",
    "error at every epoch. If you notice that the validation error\n",
    "consistently goes up, what is likely going on? How can you fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** The model might be overfitting on the train data, as the error on the train data is decreasing while error on the unseen validation data is increasing.  \n",
    "**Solution:** We can fix this using Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q4. Is it a good idea to stop Mini-batch Gradient Descent immediately\n",
    "when the validation error goes up?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** No, because the validation error might have gone up for just that specific validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q5. Suppose you are using Polynomial Regression. You plot the learning\n",
    "curves and you notice that there is a large gap between the training\n",
    "error and the validation error. What is happening? What are three\n",
    "ways to solve this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** We might be using very high degree Polynomial regression model which consequently is highly overfitting the train data.  \n",
    "**Solution:**\n",
    "1. Regularization.\n",
    "2. Using lower degree polynomial.\n",
    "3. Cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q6. Suppose you are using Ridge Regression and you notice that the\n",
    "training error and the validation error are almost equal and fairly\n",
    "high. Would you say that the model suffers from high bias or high\n",
    "variance? Should you increase the regularization hyperparameter $\\alpha$\n",
    "or reduce it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is suffering from low-bias and high-variance.  \n",
    "We should increase the regularization hyperparameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q7. Why would you want to use:**\n",
    "   - Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "       - Ans: To\n",
    "           1. Prevent model from overfitting the train data.\n",
    "           1. Keep the weights as small as possible.\n",
    "   - Lasso instead of Ridge Regression?\n",
    "       - Ans: Because\n",
    "           1. Lasso tends to make the coefficients of least important features zero, unlike ridge.\n",
    "           1. Thus Lasso regression automatically selects the important features.\n",
    "   - Elastic Net instead of Lasso?\n",
    "       - Ans: Because Lasso performs poorly when \n",
    "           1. Number of observations are less than the number of features.\n",
    "           1. Or the features are strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q8.  Can you name four of the main challenges in Machine Learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lack of quality Data.\n",
    "1. Overfitting on training data.\n",
    "1. Underfitting of training data.\n",
    "1. Non-representative training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q9. If your model performs great on the training data but generalizes\n",
    "poorly to new instances, what is happening? Can you name three\n",
    "possible solutions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has overfit on the training data and has high bias and high variance.  \n",
    "Solutions:  \n",
    "1. Cross validation.\n",
    "1. Regularization.\n",
    "1. Adding more quality data to the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q10. What is a test set, and why would you want to use it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set is the subset of the total dataset that is kept unseen from the model during training phase.  \n",
    "It is used to test the performance of the model and measure the generalization done by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q11.  What is the purpose of a validation set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set is used to get the real time performance of the model during the training phase.  \n",
    "It helps us to prevent the overfitting of the model on the train data by early stopping of the algorithm in case overfitting is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q12. What are different loss functions? Exaplain their importance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    ">Let,  \n",
    ">$n$: Number of observations.  \n",
    ">$y_i$: actual i'th observation.  \n",
    ">$\\hat{y_i}$: predicted i'th observation.\n",
    "\n",
    "Then Different loss functions can be given as:\n",
    "1. **MSE(Mean Square Error)**  \n",
    "    $\\large MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{n}$\n",
    "    - It penalizes larger error more than smaller error, because of square of the error taken.\n",
    "    - It is differentiable.\n",
    "    - It is less robust to outliers.\n",
    "1. **MAE(Mean Absolute Error)**  \n",
    "    $\\large MAE = \\frac{|y_i - \\hat{y_i}|}{n}$\n",
    "    - It is robust to outliers.\n",
    "    - It penalizes the errors by the order of their magnitudes.\n",
    "1. **Hinge Loss**  \n",
    "    $\\large L = max(0, 1-y*f(x))$\n",
    "    - It is used for classification techniques.\n",
    "    - It is non-differentiable, thus we require to use sub-gradient technique.\n",
    "1. **Cross Entropy Loss**  \n",
    "    $\\large CRL=-(y_i)log(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i})$\n",
    "    - It is used for classification techniques.\n",
    "    - It penalizes the the confident but wrong predictions the most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q13. Explain the following:**\n",
    "- **Gradient descent:**\n",
    "    - **Definition:** It is an iterative optimization technique to approximate the local minima of a function by picking up an arbitrary point.\n",
    "    - The next iterative approximation is calculated as:\n",
    "        $\\large X_{next}=X-(learningRate)*\\frac{d}{dx}f(x)$\n",
    "    - It approximates the local minima.\n",
    "    - It is highly dependent upon the choice of initial starting point.\n",
    "    - If the initial starting point is local maxima then the algorithm stops there.\n",
    "    - It has learning rate as a hyperparameter.\n",
    "    - If the learning rate is too low then it may take long time to reach local minima.\n",
    "    - If the learning rate is too high then it may offshoot the local minima.\n",
    "    - It is a good option for large datasets for which Hat matrix method is not viable.\n",
    "- **Batch gradient:**\n",
    "    - **Definition:** It is a variant of gradient descent where we use the whole training dataset for an iteration at once.\n",
    "    - It is computationally expensive.\n",
    "- **Mini-batch gradient descent:**\n",
    "    - **Definition:** It is a variant of gradient descent where we take subset or batches of the training dataset for each iteration in the epoch.\n",
    "    - It lies inbetween batch gradient descent and stochastic gradient descent.\n",
    "    - This is the often used method in practicality.\n",
    "- **Stochastic Gradient Descent:**\n",
    "    - **Definition:** It is a variant of gradient descent where we take just one train datapoint for each iteration in the epoch.\n",
    "    - It requires less computation for each iteration, but the time taken to finish an epoch is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q14. What is learning rate?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** Learning rate can be defined as hyperparameter that measures the changes in the model done by the learning algorithm for each step in response to the loss gradient.\n",
    "- When it is too small algorithm takes too much time to approximate local minima.\n",
    "- When it is too large algortithm may offshoot the local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q15. Define the following terms. Explain their importance in the data analysis.**\n",
    "- $R^2$\n",
    "- Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **R<sup>2</sup>**\n",
    "**Definition:** It is a metric to measure the performance of machine learning models which is calculated as:  \n",
    "$R^2= 1- \\frac{SS_{Residuals}}{SS_{Total}}$  \n",
    "    - It indicates to us how much proportion of the total variance is explained by a feature.\n",
    "    - It assumes that all independent features taken into consideration explains some part of the total variance.\n",
    "2. **Adjusted R<sup>2</sup>**\n",
    "**Definition:** It is the improved version of R<sup>2</sup> and is calculated as:  \n",
    "$R^2_{Adjusted} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$\n",
    "where $R^2$ is R-squared, $n$ is number of observations and $p$ is the number of features.  \n",
    "    - It is a better metric than R<sup>2</sup>\n",
    "    - It considers only those independent features which actually have affect on the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q16. Explain One-Hot Encoding and Label Encoding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **One Hot Encoding**  \n",
    "It is a technique to deal with categorical variables. In a categorical feature, for each unique category it creates a new column and fill it with 1 if value in original feature is the same category else 0.\n",
    "- For one categorical feature with 'c' categories, 'c' new columns are created for each category and the original column is removed.\n",
    "- The resulting data matrix tends to be sparse in nature.\n",
    "- When the number of categories are high the matrix becomes very sparse which in turn may lead to poor learning of the models, this is called curse of dimensionality.\n",
    "- This is nominal encoding - used when there exists no implicit ordering within the categories.\n",
    "2. **Label Encoding**  \n",
    "It is a technique to deal with categorical variables where the unique categories are mapped to unique integer values so as to make them machine readable.\n",
    "- For one categorical feature it produces one label-encoded column and removes the original column.\n",
    "- It doesnt lead to the sparse matrix typically.\n",
    "- This is ordinal encoding - used when there exists some implicit ordering among categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q17. What are the assumption on Naive Bayes algorithm in classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions of Naive Bayes algorithm:\n",
    "1. All features are independent.\n",
    "1. The prior probabilities for each class is either uniform or known beforehand empirically.\n",
    "1. The likelihood probabilities follows Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q18. What is the difference between classification and regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Regression | Classification |\n",
    "--------|------|-----------------\n",
    "|Target Variable type: | Target variable is continuous numeric type. | Target variable is discrete categorical. |\n",
    "|Output range: | There is infinite output possible typically denoted by interval in number line | The outputs are limited and discrete typically denoted by fixed numeric values. |\n",
    "|Goal: | Here we try to find the best fit line for the data. | Here we try to find the best decision boundary for different classes of data. |\n",
    "|Types:| Linear and Non-Linear Regression. | Types: Binary and multi-class classification. |\n",
    "| Algorithms: | Classical Linear Regression, Polynomial Regression, etc. | KNN,SVM, Logistic Regression,etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q19. How to ensure that the model is not overfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** We can avoid model overfitting by following ways:\n",
    "1. **Cross validation:** Whenever validation error diverges far from train error early stop the training.\n",
    "- For every epoch in the training phase train error and validation error is shown.\n",
    "- Initially naturally both are high.\n",
    "- With training both decreases.\n",
    "- When the model begins to overfit the train data, the train error keeps decreasing but validation error starts to increase.\n",
    "- The user at this point can early stop the algorithm.\n",
    "1. **Regularization:** This technique makes the non-significant coefficients weights either zero or very low.  \n",
    "For a loss function, $L(x)$ its regularized loss function is given as,  \n",
    "$\\large\\tilde{L}(x,\\lambda)=L(x)+\\lambda*(regularization Term)$, where $\\lambda$ is hyperparameter and regularization term can be lasso, ridge, etc.\n",
    "- This regularization terms basically tries to simplify the model.\n",
    "- They either decrease the weights of non-significant features very low or entirely make them zero.\n",
    "1. **Simpler Model:** In case of polynomial regression we can opt for simpler polynomial model.\n",
    "- A very high degree polynomial can create a function than can actually pass through each and every data point in the train set, thus highly overfitting train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q20. List the main advantage of Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** Advantage of Naive Bayes are:\n",
    "1. Its computation is faster compared to other techniques.\n",
    "1. If its assumptions holds true then it can outperform other techniques.\n",
    "1. It requires fewer trainind data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q21. What you shoud do when your model is suffereing from:**\n",
    "- **Low bias and high variance?**  \n",
    "This means the model has overfitted to the train data, in such case we can use cross validation technique to avoid overfitting, use less complex model.\n",
    "- **High bias and low variance?**  \n",
    "This means that the model has underfitted to the train data, we can train the model more or we can add more data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q22. What is the 'Naive' in the Naive Bayes Classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** The **assumptions** of Naive Bayes Classifier are 'naive' which are:\n",
    "1. The features are independent.\n",
    "1. The likelihood distribution is fixed as Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q23. What is bias-variance tradeoff in Machine Learning ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "**Bias:** It represents how much the model has fitted to the training data.\n",
    "- High bias = Model might have underfitted the train data.\n",
    "- Low bias = Model might have overfitted the train data.\n",
    "\n",
    "**Variance:** It represents the error of the prediction of the model in the unseen data.\n",
    "- High variance = Model might have overfitted the train data.\n",
    "- Low variance = Model might have underfitted the train data.\n",
    "\n",
    "Naturally when bias is high variance is low and when bias is low variance is high, we need to find the proper balance between the both for our model. As losing variance means gaining bias and gaining variance means losing bias we need to do the tradeoff, this is called bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q24. Explain different trade-offs in Machine Learning algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:** The different trade-offs in machine learning algorithms are:\n",
    "1. **Bias-Variance trade off:**\n",
    " This is the trade off between model's fitting appropriately to the train data, not overfitting or underfitting, while also generalizing properly.\n",
    "\n",
    "2. **Precision-Recall trade off:**\n",
    "This trade off occurs in classification algorithms.\n",
    "- **Precision:** The number of correct positive classification out of all the positive classification done.\n",
    "$Precision = \\frac{True Positive}{True Positive + False Positive}$\n",
    "- **Recall:** Out of all the actual positive how many did the model actually classified as positive.\n",
    "$Recall = \\frac{True Positive}{True Positive + False Negative}$  \n",
    "We cannot achieve high precision and high recall both at the same time, this is called precision-recall tradeoff.\n",
    "\n",
    "3. **Accuracy-Interpretability trade off:**\n",
    "There exists deep mathematical techniques which gives highly accurate predictions but has low interpretability. Example PCA.  \n",
    "It is difficult to get high accuracy model that has high interpretability for the prediction dont, this trade off is called accuracy-itnerpretability tradeoff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Q25. What is cross-validation and how it is useful in traing ML models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** Cross-validation is a technique to prevent overfitting of the model and is done as:\n",
    "1. The train data is divided into train and validation(the error response for which doesnt update the model weights) dataset.\n",
    "1. On each epoch the error on train data and error for the validation data is calculated.\n",
    "1. At the end of every epoch these errors are shown to the user.\n",
    "1. The user on detection of divergence in train error and test error can stop the algorithm early.\n",
    "\n",
    "The proper balance can ensure the termination of the training phase at the point where the model has neither underfitted nor overfitted the train data.  \n",
    "Further for each epoch the validation data acts like a test data for the model thus giving us an idea during training itself how the model would perform on the unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
